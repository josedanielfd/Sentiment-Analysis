{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using pre-trained embeddings\n",
    "\n",
    "We use GloVe pretrained embeddings on Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors) to use for sentiment analysis. We obtained the embeddings from https://nlp.stanford.edu/projects/glove/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We first import the data, and clean it by removing stopwords and tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "##### change number of imported rows!!!!!!!!!!!!!!!!!!!!!!!\n",
    "reviews = pd.read_csv('./parsed/new_data.csv')\n",
    "\n",
    "idx1 = [True if i == 1 else False for i in reviews['stars']]\n",
    "idx5 = [True if i == 5 else False for i in reviews['stars']]\n",
    "y = reviews['stars'].tolist()\n",
    "y = [-1 if star in [1,2] else 1 for star in y]\n",
    "reviews = pd.Series(reviews['text'])\n",
    "reviews = reviews.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "##### change number of imported rows!!!!!!!!!!!!!!!!!!!!!!!\n",
    "reviews = pd.read_csv('./parsed/review_db.gz', compression='gzip', nrows=None, \n",
    "                      skiprows=[1871819, 3304881], dtype={'stars': np.int8, 'text': str})\n",
    "idx1 = [True if i == 1 else False for i in reviews['stars']]\n",
    "idx5 = [True if i == 5 else False for i in reviews['stars']]\n",
    "reviews = reviews[idx1].sample(n=10000).append(reviews[idx5].sample(n=10000))\n",
    "y = reviews['stars'].tolist()\n",
    "reviews = pd.Series(reviews.iloc[:,1])\n",
    "reviews = reviews.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "useless staff order correct reasonable time frame management need rehire staff\n",
      "1\n",
      "go boyfriend week order sizzle plate special fish   assorted seafood boil season salty compare near yonge college fish really salty teriyaki sauce will not go location hot like really try hard kitchen salty season shrimp devein bring boyfriend server eye boyfriend nerve gently touch handing debit machine lol hope help\n",
      "2\n",
      "go fountain attach recess order burger look pretty good absolutely cover sauce personally awful list menu burger like completely swim stuff take bite burger like taste sauce basically mayonnaise sandwich server return later ask alright explain sauce bad quietly apologized take plate away return minute later billin service mediocre inattentive unpleasant hurry despite table restaurant end pay leave unsatisfiedwill back\n",
      "3\n",
      "agree employee tim horton location bit rude today 10 pm   jan 16 2016 go tim horton order hot chocolate white female extremely rude racist give attitude handle drink rough manner sort roughly place counter see interaction white guy line polite friendly thinking partthat korea town majority customer minority maybe work tim horton small town there s racist people like live regular customer compare location near uoft compareable cauasian girl work uoft location respectful trained mannerism customer service\n",
      "4\n",
      "come rocknes year time issue hostess completely totally ridiculous wife somebody complain restaurant call manager explain exactly happen response shrug shoulder say sorry walk away customer service back\n"
     ]
    }
   ],
   "source": [
    "# load spacy en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# load stopwords\n",
    "# inspired by https://stackoverflow.com/a/3510894\n",
    "stopwords = pd.read_csv(\"./nlp/stopwords.txt\", header = None, encoding = \"UTF-8\", squeeze=True)\n",
    "stopwords = np.asarray(stopwords)\n",
    "pattern = \"|\".join(stopwords)\n",
    "stopwords = re.compile(\"\\\\b(\" + pattern + \")\\\\W\", re.I)\n",
    "punctuation = re.compile(r'[^\\w_ ]+', re.I)\n",
    "\n",
    "# clean the text (drops punctuation, only alpha-numeric, no stopwords)\n",
    "x = []\n",
    "i = 0\n",
    "for review in reviews:\n",
    "    review = punctuation.sub(\"\", review)\n",
    "    review = stopwords.sub(\"\", review)\n",
    "    parsed_text = nlp(review)\n",
    "    intermediate_text = []\n",
    "    for token in parsed_text:\n",
    "        intermediate_text.append(token.lemma_.lower())\n",
    "    words = \" \".join(intermediate_text)\n",
    "    if words != '':\n",
    "        x.append(words)\n",
    "    i+=1\n",
    "    \n",
    "i=0\n",
    "for review in x[:5]:\n",
    "    print(i)\n",
    "    print(review)\n",
    "    i+=1\n",
    "    \n",
    "# clear memory\n",
    "reviews = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features\n",
    "We then turn the Yelp reviews into vectors with the help of word embeddings. These embeddings are pre-trained tensors of the GloVe model on Twitter. So, we load these embeddings and create an average of the word tensors by review. This is, given a review, we take the average of the individual token embeddings and asign it to the review. Hence, each Yelp review now is has a feature vector built on the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation learning for very short texts using weighted word embedding aggregation. Cedric De Boom, Steven Van Canneyt, Thomas Demeester, Bart Dhoedt. Pattern Recognition Letters; arxiv:1607.00570. abstract, pdf. See especially Tables 1 and 2. https://arxiv.org/pdf/1607.00570"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  50 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load GloVe\n",
    "# Pre-process the embeddings\n",
    "embeddings_index = {}\n",
    "\n",
    "# We will use the 50-dimensional embedding vectors\n",
    "with open(\"./nlp/glove.twitter.27B.50d.txt\", encoding='UTF-8') as f:\n",
    "    # Each row represents a word vector\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # The first part is word\n",
    "        word = values[0]\n",
    "        # The rest are the embedding vector\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding feature matrix by attaching average embedding to each review\n",
    "fmatrix = np.zeros((len(x), 150))\n",
    "ave_matrix = np.zeros((len(x), 50))\n",
    "j = 0\n",
    "for review in x:\n",
    "    words = review.split()\n",
    "    unique_words = np.unique(words)\n",
    "    embedding_matrix = np.zeros((len(words), 50))\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            i+=1\n",
    "    # average\n",
    "    fmatrix[j] = np.append(embedding_matrix.max(0), [embedding_matrix.min(0), embedding_matrix.mean(0)])\n",
    "    ave_matrix[j] = embedding_matrix.mean(0)\n",
    "    j += 1\n",
    "\n",
    "# clear memory\n",
    "# x = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  100 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load GloVe\n",
    "# Pre-process the embeddings\n",
    "embeddings_index = {}\n",
    "\n",
    "# We will use the 50-dimensional embedding vectors\n",
    "with open(\"./nlp/glove.twitter.27B.100d.txt\", encoding='UTF-8') as f:\n",
    "    # Each row represents a word vector\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # The first part is word\n",
    "        word = values[0]\n",
    "        # The rest are the embedding vector\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding feature matrix by attaching average embedding to each review\n",
    "fmatrix100 = np.zeros((len(x), 300))\n",
    "ave_matrix100 = np.zeros((len(x), 100))\n",
    "j = 0\n",
    "for review in x:\n",
    "    words = review.split()\n",
    "    unique_words = np.unique(words)\n",
    "    embedding_matrix = np.zeros((len(words), 100))\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            i+=1\n",
    "    # average\n",
    "    fmatrix100[j] = np.append(embedding_matrix.max(0), [embedding_matrix.min(0), embedding_matrix.mean(0)])\n",
    "    ave_matrix100[j] = embedding_matrix.mean(0)\n",
    "    j += 1\n",
    "\n",
    "# clear memory\n",
    "# x = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "We now train several models using the review embeddings as feature matrix. We run a grid search over the models to select the best model, and the best parametrization for every model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  50 dimensional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# split data into testing and training\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(ave_matrix, y, test_size=0.25, random_state=11)\n",
    "\n",
    "models = [\n",
    "         \"Naive Bayes\",\n",
    "         \"SVM\",\n",
    "         \"Logistic Regression\",\n",
    "         \"Random Forest\",\n",
    "         \"Adaboost Forest\"\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    SVC(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "              {},                                                         # Gaussian Naive Bayes\n",
    "              {'C': np.logspace(-4, 3, 5),                                # SVM\n",
    "               'degree': range(1, 4),\n",
    "               'kernel': ['poly']},\n",
    "              {'C': np.logspace(-4, 3, 15)},                              # Logistic Regression\n",
    "              {'n_estimators': np.linspace(100, 1000, 5).astype('int')},  # Random Forest\n",
    "              {'n_estimators': np.linspace(100, 1500, 5).astype('int'),    # Adaboost Forest\n",
    "               'learning_rate': np.linspace(0.001, 2, 3)},\n",
    "             ]\n",
    "optimal = {}\n",
    "for model, clf, params in zip(models, classifiers, parameters):\n",
    "    gscv = GridSearchCV(clf, param_grid=params, cv=10, n_jobs=-1)\n",
    "    gscv = gscv.fit(xtrain, ytrain)\n",
    "    score = gscv.best_score_\n",
    "    optimal[model] = {'clf':gscv.best_estimator_, 'score':score}\n",
    "    print(\"{} score: {}\".format(model, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.50075, n_estimators=537, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal[\"Adaboost Forest\"]['clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max, Min, Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Naive Bayes score: 0.6897333333333333\n",
      "SVM score: 0.8569333333333333\n",
      "Logistic Regression score: 0.8556\n",
      "Random Forest score: 0.8326666666666667\n",
      "Adaboost Forest score: 0.8498666666666667\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# split data into testing and training\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(fmatrix, y, test_size=0.25, random_state=11)\n",
    "\n",
    "models = [\n",
    "         \"Naive Bayes\",\n",
    "         \"SVM\",\n",
    "         \"Logistic Regression\",\n",
    "         \"Random Forest\",\n",
    "         \"Adaboost Forest\"\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    SVC(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "              {},                                                         # Gaussian Naive Bayes\n",
    "              {'C': np.logspace(-4, 3, 5),                                # SVM\n",
    "               'degree': range(1, 4),\n",
    "               'kernel': ['poly']},\n",
    "              {'C': np.logspace(-4, 3, 15)},                              # Logistic Regression\n",
    "              {'n_estimators': np.linspace(100, 1000, 5).astype('int')},  # Random Forest\n",
    "              {'n_estimators': np.linspace(100, 1500, 5).astype('int'),    # Adaboost Forest\n",
    "               'learning_rate': np.linspace(0.001, 2, 3)},\n",
    "             ]\n",
    "optimal = {}\n",
    "for model, clf, params in zip(models, classifiers, parameters):\n",
    "    gscv = GridSearchCV(clf, param_grid=params, cv=10, n_jobs=-1)\n",
    "    gscv = gscv.fit(xtrain, ytrain)\n",
    "    score = gscv.best_score_\n",
    "    optimal[model] = {'clf':gscv.best_estimator_, 'score':score}\n",
    "    print(\"{} score: {}\".format(model, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.50075, n_estimators=537, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal[\"Adaboost Forest\"]['clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  100 dimensional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# split data into testing and training\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(ave_matrix100, y, test_size=0.25, random_state=11)\n",
    "\n",
    "models = [\n",
    "         \"Naive Bayes\",\n",
    "         \"SVM\",\n",
    "         \"Logistic Regression\",\n",
    "         \"Random Forest\",\n",
    "         \"Adaboost Forest\"\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    SVC(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "              {},                                                         # Gaussian Naive Bayes\n",
    "              {'C': np.logspace(-4, 3, 5),                                # SVM\n",
    "               'degree': range(1, 4),\n",
    "               'kernel': ['poly']},\n",
    "              {'C': np.logspace(-4, 3, 15)},                              # Logistic Regression\n",
    "              {'n_estimators': np.linspace(100, 1000, 5).astype('int')},  # Random Forest\n",
    "              {'n_estimators': np.linspace(100, 1500, 5).astype('int'),    # Adaboost Forest\n",
    "               'learning_rate': np.linspace(0.001, 2, 3)},\n",
    "             ]\n",
    "optimal = {}\n",
    "for model, clf, params in zip(models, classifiers, parameters):\n",
    "    gscv = GridSearchCV(clf, param_grid=params, cv=10, n_jobs=-1)\n",
    "    gscv = gscv.fit(xtrain, ytrain)\n",
    "    score = gscv.best_score_\n",
    "    optimal[model] = {'clf':gscv.best_estimator_, 'score':score}\n",
    "    print(\"{} score: {}\".format(model, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.50075, n_estimators=537, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal[\"Adaboost Forest\"]['clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max, Min, Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Naive Bayes score: 0.6897333333333333\n",
      "SVM score: 0.8569333333333333\n",
      "Logistic Regression score: 0.8556\n",
      "Random Forest score: 0.8326666666666667\n",
      "Adaboost Forest score: 0.8498666666666667\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# split data into testing and training\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(fmatrix100, y, test_size=0.25, random_state=11)\n",
    "\n",
    "models = [\n",
    "         \"Naive Bayes\",\n",
    "         \"SVM\",\n",
    "         \"Logistic Regression\",\n",
    "         \"Random Forest\",\n",
    "         \"Adaboost Forest\"\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    SVC(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "              {},                                                         # Gaussian Naive Bayes\n",
    "              {'C': np.logspace(-4, 3, 5),                                # SVM\n",
    "               'degree': range(1, 4),\n",
    "               'kernel': ['poly']},\n",
    "              {'C': np.logspace(-4, 3, 15)},                              # Logistic Regression\n",
    "              {'n_estimators': np.linspace(100, 1000, 5).astype('int')},  # Random Forest\n",
    "              {'n_estimators': np.linspace(100, 1500, 5).astype('int'),    # Adaboost Forest\n",
    "               'learning_rate': np.linspace(0.001, 2, 3)},\n",
    "             ]\n",
    "optimal = {}\n",
    "for model, clf, params in zip(models, classifiers, parameters):\n",
    "    gscv = GridSearchCV(clf, param_grid=params, cv=10, n_jobs=-1)\n",
    "    gscv = gscv.fit(xtrain, ytrain)\n",
    "    score = gscv.best_score_\n",
    "    optimal[model] = {'clf':gscv.best_estimator_, 'score':score}\n",
    "    print(\"{} score: {}\".format(model, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.50075, n_estimators=537, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal[\"Adaboost Forest\"]['clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate test error\n",
    "We now test the best model, as obtained by the last procedure. This model is the best among the selected model families and also has the best hyper-parameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the best model\n",
    "bestModel = max(map(lambda x: (optimal[x]['score'], x), optimal.keys()))[1]\n",
    "bestClf = optimal[bestModel]['clf']\n",
    "\n",
    "# calculate the predictions\n",
    "predtest = bestClf.predict(xtest)\n",
    "predtrain = bestClf.predict(xtrain)\n",
    "\n",
    "# report training and test accuracy\n",
    "print('Training accuracy is: ' + str(accuracy_score(ytrain, predtrain)))\n",
    "print('Test accuracy is: ' + str(accuracy_score(ytest, predtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
